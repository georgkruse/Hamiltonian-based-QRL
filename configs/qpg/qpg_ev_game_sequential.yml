type:                        QRL                   # choose a type: RL, GA, Hypernetwork, ...
alg:                         QPG                  # name of the algorithm
use_ray_alg:                 False                 # use custom algorithm or ray framework
seed:                        42                   # seed for ray alogorithms/tensorflow/pytorch for reproduceabel results

mode:                        training             # select the mode you want to use: 'custom', 'training'
trainer:                     default
checkpoint_at_end:           False
checkpoint_freq:             50
ray_local_mode:              False                # set local_mode of ray to True for debugging
load_path:                   some/path/weights    # provide a model weight path if you want to do continue from stopped training

ray_logging_path:            logs/qpg/ev/sequential/graph_encoding    # logging directory
total_num_cpus:              12                    # total number of cpus
total_num_gpus:              0                    # total number of gpus
ray_num_trial_samples:       3                    # number of hyperparameter combinations (trials) to run (works differently for grid_search)
training_iterations:         150                 # number of training iterations


env_type:                     gym_game
env:                          EVQUBOSEQUENTIAL
env_config:
  number_cars:                5
  number_timesteps:           5
  u:                          [[1,1,0,0,1],[0,1,1,0,0],[1,0,1,0,0],[1,1,0,1,1],[0,0,1,1,1]]
  v:                          [[5,5,0,0,5],[0,10,10,0,0],[15,0,15,0,0],[5,5,0,5,5],[0,0,10,10,10]]
  max_capacity:               2
  max_power:                  20
  lambda_1:                   3.54
  lambda_2:                   0.21
  use_capacity_constraints:   True
  use_power_constraints:      True
  reward_type:                problem
  negative_reward_value:      10

algorithm_config:
  reuse_actors:               True
  num_gpus:                   0                   # number of gpus for each trial
  num_rollout_workers:        1                  # number of worker for each trial. By ray default, every worker needs one cpu
  num_envs_per_worker:        1                   # number of game enviroments for each worker
  num_gpus_per_worker:        0                   # number of worker for each trial.
  num_cpus_per_worker:        2                # number of worker for each trial.
  constant_seed:              False 
  framework:                  torch
  ###########################################################################
  lr:                         0.025
    # - grid_search
    # - float
    # - [0.0025, 0.001, 0.00075, 0.0005] 
  # weight_decay: 
  #   - grid_search
  #   - float
  #   - [0, 1.0e-4, 1.0e-5]
  lr_output_scaling:          0.1
    # - grid_search
    # - float
    # - [0.1, 0.01]  
  num_layers:                5
  # blocks: 
  #   - choice 
  #   - string
  #   - [[[[0, 1], [2, 3]], [[0, 1], [2], [3]]]]
  ###########################################################################
  mode:                       quantum # classical, quantum, hybrid
    # - grid_search
    # - string
    # - [classical, quantum]
  interface:                  torch
  diff_method:                adjoint                             
  backend_name:               lightning.qubit
  custom_optimizer:           Adam   
  ###########################################################################
  vqc_type:                   [vqc_generator, 5] #qcnn_circuit_pendulum # triple_circuit
  use_hadamard:               True
  block_sequence:             enc
  encoding_type:              graph_encoding #angular_classical # layerwise_arctan_sigmoid #angular_classical
  graph_encoding_type:        #eqc
    - grid_search
    - string
    - [eqc,neqc,linear-eqc,linear-neqc]
  use_input_scaling:          True
  num_scaling_params:         4
  entangling_type:            chain
  entangling_gate:            CNOT
  variational_type:           RZ_RY
  num_variational_params:     2
  measurement_type_actor:     exp_@_exp             # type of measurement (check the python files for examples) (exp for discrete) exp_@_exp+exp
  use_output_scaling_actor:   True
  init_output_scaling_actor:  [1.]
  postprocessing_actor:       1
  noise:
    coherent:                 [False, 0.]
    depolarizing:             [False, 0.001]
  ###########################################################################
  init_params:                0.1
  init_params_mode:           plus-zero-uniform    # plus-zero-uniform #, plus-plus-normal. plus-zero-normal
  layerwise_training:         False
  gradient_clipping:          False
  use_classical_layer:        False
  layer_size:                 [64, 64]
  activation_function:        relu      
  weight_logging_interval:    5000
  weight_plotting:            False
  ###########################################################################
  # More ray params
  explore:                    True
    # - grid_search
    # - string
    # - [False, True]

  