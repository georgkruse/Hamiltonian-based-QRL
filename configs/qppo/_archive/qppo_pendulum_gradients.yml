type:                        QRL                   # choose a type: RL, GA, Hypernetwork, ...
alg:                         QPPO                  # name of the algorithm
use_ray_alg:                 False                 # use custom algorithm or ray framework
seed:                        42                   # seed for ray alogorithms/tensorflow/pytorch for reproduceabel results

mode:                        training             # select the mode you want to use: 'custom', 'training'
trainer:                     default
checkpoint_at_end:           False
checkpoint_freq:             100
ray_local_mode:              False                # set local_mode of ray to True for debugging
load_path:                   some/path/weights    # provide a model weight path if you want to do continue from stopped training

ray_logging_path:            logs/trainability/pendulum/06_27       # logging directory
total_num_cpus:              12                 # total number of cpus
total_num_gpus:              0                    # total number of gpus
ray_num_trial_samples:       3                    # number of hyperparameter combinations (trials) to run (works differently for grid_search)
training_iterations:         30                  # number of training iterations

# Evaluation parameters
evaluation:
  eval:                      False                 # perform evaluation or not (True or False).
  evaluation_interval:       1                    # interval to perform evaluation. If interval 2, it will follow as train, train, eval, train... 
  evaluation_num_workers:    1
  evalutation_iteration:     100
  render:                    True                 # render results of evaluation (only for custom evalution fuction).
  custom_function:           False                # use custom evaluation function or not. 

env_type:                     gym_game
env:                          Pendulum-v1
env_config:
  mode:                       classical
data_path: /home/users/kruse/quantum-computing/QRL/pendulum_data_2.csv

# Additional custom parameters
# withOUT rescaling of critic 
algorithm_config:
  reuse_actors:             True
  num_gpus:                   0                   # number of gpus for each trial
  num_workers:                1                   # number of worker for each trial. By ray default, every worker needs one cpu
  num_envs_per_worker:        1                   # number of game enviroments for each worker
  num_gpus_per_worker:        0                   # number of worker for each trial.
  num_cpus_per_worker:        1                # number of worker for each trial.
  constant_seed:              False 
  ###########################################################################
  lr:                         0.01
    # - grid_search
    # - [0.01, 0.001, 0.0001] 
  lr_output_scaling:          0.1
    # - grid_search
    # - [0.1, 0.01, 0.001]  
  num_layers:                 3          
  num_qubits:                 4
  vqc: no_reupload_relu_chain_ent_6
  ###########################################################################
  mode:                       quantum # classical, quantum, hybrid
  interface:                  torch
  diff_method:                adjoint                             
  backend_name:               lightning.qubit
  ###########################################################################

  vqc_type:                   relu_circuit #vqc_generator 
  use_single_vqc:             False
    # - grid_search
    # - [True, False]
  use_hadamard:               True
  block_sequence:             var_ent
  encoding_type:              angular_classical
    # - grid_search
    # - [angular_classical, angular_arctan]
  use_input_scaling:          True
  num_scaling_params:         2
  entangling_type:            chain
    # - grid_search
    # - [full, chain]
  entangling_gate:           CZ 
    # - grid_search
    # - [CZ, CNOT]
  variational_type:           RZ_RY
  num_variational_params:     2
  measurement_type_actor:     exp             # type of measurement (check the python files for examples) (exp for discrete)
  measurement_type_critic:    exp
    # - grid_search
    # - [exp, exp_@_exp]
  use_output_scaling_actor:   True
  init_output_scaling_actor:  [1.]
  use_output_scaling_critic:  True                     # Uses output scaling  
  init_output_scaling_critic: [1.] 
  postprocessing:             0
  postprocessing_critic:      weighted_sum
  ###########################################################################
  init_params: 1.57               
    # - grid_search 
    # - [1.57, 0.1] 
  init_params_mode:           plus-zero-uniform    # plus-zero-uniform #, plus-plus-normal. plus-zero-normal
    # - grid_search
    # - [plus-zero-uniform, plus-zero-normal]
  layerwise_training:         False
  gradient_clipping:          False
  use_classical_layer:        False
  layer_size:                 2
  weight_logging_interval:    10000
  weight_plotting:            False
  ###########################################################################
  # More ray params
  # clip_param: 1
  #   # - grid_search
  #   # - [0.5, 1.]
  # vf_clip_param : 100
  train_batch_size:           4000
    #  - grid_search
    #  - [400, 800]
  num_sgd_iter:               10
  sgd_minibatch_size:         64
  rollout_fragment_length:    200
  gamma:                      0.95
  lambda:                     0.1
  kl_coeff:                   1.0
  # kl_target:                  0.05
  vf_loss_coeff:              0.5
  clip_param:                 0.2
  # grad_clip:                  1
  vf_clip_param:              10000.



