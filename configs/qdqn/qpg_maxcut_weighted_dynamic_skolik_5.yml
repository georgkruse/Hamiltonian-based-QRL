type:                             QRL                   # choose a type: RL, GA, Hypernetwork (DEPRECATED)
alg:                              QDQN                   # name of the algorithm
seed:                             42                    # seed for ray alogorithms/tensorflow/pytorch for reproduceabel results
constant_seed:                    False                 # setting the same seed accross all runs
checkpoint_at_end:                False                 # create checkpoint at the end of training
checkpoint_freq:                  1000                  # set checkpoint frequency, depends on training iterations
ray_local_mode:                   True                 # set local_mode of ray to True for debugging
ray_logging_path:                 logs/maxcut_weighted/dynamic/5nodes/complete_graph/skolik/q_learning/test         # logging directory
total_num_cpus:                   15                    # total number of cpus
total_num_gpus:                   0                     # total number of gpus
ray_num_trial_samples:            3                     # number of hyperparameter combinations (trials) to run (works differently for grid_search)
training_iterations:              500                   # number of training iterations
run_sections:                                           # specifiy the code you want to run
                                - qrl_training
                                #- plotting
                                # - landscape_plotting
                                # - algorithm_benchmarking
                                # - variance_calculation

###########################################################################
env:                              MAXCUTWEIGHTEDDYNAMIC                        # Game config               
env_config:
  nodes:                      5
  graph:                      [[[0, 1, 0.73], [0, 2, 0.3], [0, 3, 0.23], [0, 4, 0.64], [1, 2, 0.92], [1, 3, 0.55], [1, 4, 0.17], [2, 3, 0.52], [2, 4, 0.87], [3, 4, 0.43]], [[0, 1, 0.68], [0, 2, 0.14], [0, 3, 0.49], [0, 4, 0.93], [1, 2, 0.54], [1, 3, 0.96], [1, 4, 0.59], [2, 3, 0.56], [2, 4, 0.83], [3, 4, 0.9]], [[0, 1, 0.31], [0, 2, 0.92], [0, 3, 0.78], [0, 4, 0.31], [1, 2, 0.71], [1, 3, 0.59], [1, 4, 0.12], [2, 3, 0.14], [2, 4, 0.63], [3, 4, 0.51]], [[0, 1, 0.75], [0, 2, 0.46], [0, 3, 0.65], [0, 4, 0.96], [1, 2, 0.68], [1, 3, 0.21], [1, 4, 0.22], [2, 3, 0.71], [2, 4, 0.89], [3, 4, 0.34]], [[0, 1, 0.63], [0, 2, 0.5], [0, 3, 0.33], [0, 4, 0.33], [1, 2, 0.8], [1, 3, 0.51], [1, 4, 0.82], [2, 3, 0.3], [2, 4, 0.81], [3, 4, 0.66]], [[0, 1, 0.24], [0, 2, 0.27], [0, 3, 0.53], [0, 4, 0.5], [1, 2, 0.03], [1, 3, 0.76], [1, 4, 0.72], [2, 3, 0.79], [2, 4, 0.75], [3, 4, 0.1]], [[0, 1, 0.39], [0, 2, 0.73], [0, 3, 0.3], [0, 4, 0.72], [1, 2, 0.35], [1, 3, 0.54], [1, 4, 0.38], [2, 3, 0.82], [2, 4, 0.16], [3, 4, 0.75]], [[0, 1, 0.5], [0, 2, 0.84], [0, 3, 0.71], [0, 4, 1.03], [1, 2, 0.74], [1, 3, 0.29], [1, 4, 0.66], [2, 3, 0.57], [2, 4, 0.5], [3, 4, 0.37]], [[0, 1, 0.55], [0, 2, 0.77], [0, 3, 0.66], [0, 4, 0.19], [1, 2, 0.93], [1, 3, 0.73], [1, 4, 0.69], [2, 3, 0.21], [2, 4, 0.93], [3, 4, 0.83]], [[0, 1, 0.59], [0, 2, 0.62], [0, 3, 0.29], [0, 4, 0.46], [1, 2, 0.06], [1, 3, 0.29], [1, 4, 0.18], [2, 3, 0.32], [2, 4, 0.18], [3, 4, 0.18]], [[0, 1, 0.68], [0, 2, 0.18], [0, 3, 0.72], [0, 4, 0.88], [1, 2, 0.67], [1, 3, 0.94], [1, 4, 0.32], [2, 3, 0.54], [2, 4, 0.81], [3, 4, 0.88]], [[0, 1, 0.72], [0, 2, 0.13], [0, 3, 0.22], [0, 4, 0.3], [1, 2, 0.84], [1, 3, 0.58], [1, 4, 0.42], [2, 3, 0.35], [2, 4, 0.42], [3, 4, 0.19]], [[0, 1, 0.31], [0, 2, 0.27], [0, 3, 0.48], [0, 4, 0.49], [1, 2, 0.57], [1, 3, 0.5], [1, 4, 0.43], [2, 3, 0.55], [2, 4, 0.62], [3, 4, 0.13]], [[0, 1, 0.63], [0, 2, 0.37], [0, 3, 0.3], [0, 4, 0.32], [1, 2, 0.98], [1, 3, 0.92], [1, 4, 0.93], [2, 3, 0.07], [2, 4, 0.05], [3, 4, 0.03]], [[0, 1, 0.88], [0, 2, 0.57], [0, 3, 0.56], [0, 4, 0.28], [1, 2, 0.38], [1, 3, 0.91], [1, 4, 0.77], [2, 3, 0.54], [2, 4, 0.58], [3, 4, 0.78]], [[0, 1, 0.34], [0, 2, 0.57], [0, 3, 0.48], [0, 4, 0.49], [1, 2, 0.65], [1, 3, 0.69], [1, 4, 0.3], [2, 3, 0.25], [2, 4, 0.46], [3, 4, 0.61]], [[0, 1, 0.89], [0, 2, 0.1], [0, 3, 0.25], [0, 4, 0.85], [1, 2, 0.92], [1, 3, 0.65], [1, 4, 0.17], [2, 3, 0.3], [2, 4, 0.87], [3, 4, 0.62]], [[0, 1, 0.69], [0, 2, 0.23], [0, 3, 0.6], [0, 4, 0.6], [1, 2, 0.78], [1, 3, 0.5], [1, 4, 0.96], [2, 3, 0.52], [2, 4, 0.37], [3, 4, 0.52]], [[0, 1, 0.19], [0, 2, 0.19], [0, 3, 0.83], [0, 4, 0.52], [1, 2, 0.38], [1, 3, 0.97], [1, 4, 0.7], [2, 3, 0.66], [2, 4, 0.33], [3, 4, 0.4]], [[0, 1, 0.91], [0, 2, 0.38], [0, 3, 0.78], [0, 4, 0.12], [1, 2, 0.53], [1, 3, 0.14], [1, 4, 0.83], [2, 3, 0.4], [2, 4, 0.31], [3, 4, 0.69]], [[0, 1, 0.27], [0, 2, 0.58], [0, 3, 0.84], [0, 4, 0.12], [1, 2, 0.69], [1, 3, 0.89], [1, 4, 0.15], [2, 3, 0.29], [2, 4, 0.61], [3, 4, 0.85]], [[0, 1, 0.68], [0, 2, 0.76], [0, 3, 0.42], [0, 4, 0.24], [1, 2, 0.11], [1, 3, 1.0], [1, 4, 0.7], [2, 3, 1.1], [2, 4, 0.8], [3, 4, 0.31]], [[0, 1, 0.35], [0, 2, 0.52], [0, 3, 0.68], [0, 4, 0.35], [1, 2, 0.74], [1, 3, 1.01], [1, 4, 0.67], [2, 3, 0.47], [2, 4, 0.62], [3, 4, 0.5]], [[0, 1, 0.67], [0, 2, 0.64], [0, 3, 0.78], [0, 4, 0.62], [1, 2, 0.43], [1, 3, 0.18], [1, 4, 0.37], [2, 3, 0.35], [2, 4, 0.05], [3, 4, 0.31]], [[0, 1, 1.12], [0, 2, 0.73], [0, 3, 0.93], [0, 4, 0.63], [1, 2, 0.39], [1, 3, 0.47], [1, 4, 0.75], [2, 3, 0.43], [2, 4, 0.44], [3, 4, 0.86]], [[0, 1, 0.38], [0, 2, 0.13], [0, 3, 0.74], [0, 4, 0.28], [1, 2, 0.33], [1, 3, 0.94], [1, 4, 0.38], [2, 3, 0.86], [2, 4, 0.37], [3, 4, 0.56]], [[0, 1, 0.8], [0, 2, 0.39], [0, 3, 0.66], [0, 4, 0.23], [1, 2, 0.41], [1, 3, 0.49], [1, 4, 0.84], [2, 3, 0.41], [2, 4, 0.45], [3, 4, 0.55]], [[0, 1, 0.86], [0, 2, 0.39], [0, 3, 0.53], [0, 4, 0.8], [1, 2, 0.48], [1, 3, 0.72], [1, 4, 0.55], [2, 3, 0.39], [2, 4, 0.5], [3, 4, 0.34]], [[0, 1, 0.57], [0, 2, 0.18], [0, 3, 0.2], [0, 4, 0.91], [1, 2, 0.7], [1, 3, 0.77], [1, 4, 0.62], [2, 3, 0.18], [2, 4, 0.92], [3, 4, 1.08]], [[0, 1, 0.78], [0, 2, 0.38], [0, 3, 0.29], [0, 4, 0.84], [1, 2, 0.84], [1, 3, 0.58], [1, 4, 0.07], [2, 3, 0.27], [2, 4, 0.91], [3, 4, 0.65]], [[0, 1, 0.11], [0, 2, 0.06], [0, 3, 0.71], [0, 4, 0.73], [1, 2, 0.17], [1, 3, 0.79], [1, 4, 0.84], [2, 3, 0.68], [2, 4, 0.67], [3, 4, 0.52]], [[0, 1, 0.52], [0, 2, 0.85], [0, 3, 0.71], [0, 4, 0.7], [1, 2, 0.33], [1, 3, 0.31], [1, 4, 0.26], [2, 3, 0.3], [2, 4, 0.25], [3, 4, 0.06]], [[0, 1, 0.76], [0, 2, 0.64], [0, 3, 0.69], [0, 4, 0.33], [1, 2, 0.27], [1, 3, 0.53], [1, 4, 0.64], [2, 3, 0.26], [2, 4, 0.43], [3, 4, 0.38]], [[0, 1, 0.18], [0, 2, 0.77], [0, 3, 0.55], [0, 4, 0.61], [1, 2, 0.87], [1, 3, 0.63], [1, 4, 0.68], [2, 3, 0.25], [2, 4, 0.2], [3, 4, 0.06]], [[0, 1, 0.13], [0, 2, 0.5], [0, 3, 0.69], [0, 4, 0.32], [1, 2, 0.56], [1, 3, 0.82], [1, 4, 0.37], [2, 3, 0.68], [2, 4, 0.19], [3, 4, 0.7]], [[0, 1, 0.65], [0, 2, 0.32], [0, 3, 0.18], [0, 4, 0.93], [1, 2, 0.36], [1, 3, 0.48], [1, 4, 0.4], [2, 3, 0.14], [2, 4, 0.71], [3, 4, 0.79]], [[0, 1, 0.95], [0, 2, 0.49], [0, 3, 0.74], [0, 4, 1.06], [1, 2, 0.81], [1, 3, 0.66], [1, 4, 0.63], [2, 3, 0.29], [2, 4, 0.63], [3, 4, 0.35]], [[0, 1, 0.53], [0, 2, 0.26], [0, 3, 0.67], [0, 4, 0.24], [1, 2, 0.78], [1, 3, 0.63], [1, 4, 0.72], [2, 3, 0.73], [2, 4, 0.1], [3, 4, 0.62]], [[0, 1, 0.15], [0, 2, 0.3], [0, 3, 0.83], [0, 4, 0.68], [1, 2, 0.37], [1, 3, 0.93], [1, 4, 0.78], [2, 3, 0.56], [2, 4, 0.42], [3, 4, 0.17]], [[0, 1, 0.54], [0, 2, 0.46], [0, 3, 0.52], [0, 4, 0.36], [1, 2, 0.42], [1, 3, 0.78], [1, 4, 0.53], [2, 3, 0.93], [2, 4, 0.17], [3, 4, 0.87]], [[0, 1, 0.83], [0, 2, 0.66], [0, 3, 0.76], [0, 4, 0.85], [1, 2, 0.28], [1, 3, 0.22], [1, 4, 0.15], [2, 3, 0.41], [2, 4, 0.21], [3, 4, 0.37]], [[0, 1, 0.72], [0, 2, 0.32], [0, 3, 0.66], [0, 4, 0.89], [1, 2, 0.52], [1, 3, 0.89], [1, 4, 0.54], [2, 3, 0.87], [2, 4, 0.89], [3, 4, 0.61]], [[0, 1, 0.23], [0, 2, 0.61], [0, 3, 0.36], [0, 4, 0.38], [1, 2, 0.79], [1, 3, 0.59], [1, 4, 0.6], [2, 3, 0.37], [2, 4, 0.28], [3, 4, 0.1]], [[0, 1, 0.45], [0, 2, 0.39], [0, 3, 0.56], [0, 4, 0.4], [1, 2, 0.52], [1, 3, 0.56], [1, 4, 0.81], [2, 3, 0.19], [2, 4, 0.73], [3, 4, 0.93]], [[0, 1, 0.24], [0, 2, 0.6], [0, 3, 0.81], [0, 4, 0.18], [1, 2, 0.48], [1, 3, 0.58], [1, 4, 0.36], [2, 3, 0.5], [2, 4, 0.54], [3, 4, 0.86]], [[0, 1, 0.55], [0, 2, 0.15], [0, 3, 0.56], [0, 4, 0.23], [1, 2, 0.69], [1, 3, 0.89], [1, 4, 0.68], [2, 3, 0.49], [2, 4, 0.25], [3, 4, 0.74]], [[0, 1, 0.57], [0, 2, 0.14], [0, 3, 0.62], [0, 4, 0.82], [1, 2, 0.56], [1, 3, 0.27], [1, 4, 0.82], [2, 3, 0.55], [2, 4, 0.69], [3, 4, 0.56]], [[0, 1, 0.14], [0, 2, 0.44], [0, 3, 0.37], [0, 4, 0.91], [1, 2, 0.51], [1, 3, 0.51], [1, 4, 0.89], [2, 3, 0.37], [2, 4, 0.6], [3, 4, 0.97]], [[0, 1, 0.02], [0, 2, 0.44], [0, 3, 0.54], [0, 4, 0.62], [1, 2, 0.46], [1, 3, 0.56], [1, 4, 0.61], [2, 3, 0.1], [2, 4, 0.73], [3, 4, 0.81]], [[0, 1, 0.15], [0, 2, 0.22], [0, 3, 0.86], [0, 4, 0.13], [1, 2, 0.26], [1, 3, 0.76], [1, 4, 0.03], [2, 3, 0.71], [2, 4, 0.23], [3, 4, 0.75]], [[0, 1, 0.83], [0, 2, 0.48], [0, 3, 0.18], [0, 4, 0.3], [1, 2, 0.68], [1, 3, 0.78], [1, 4, 0.55], [2, 3, 0.31], [2, 4, 0.32], [3, 4, 0.24]], [[0, 1, 0.47], [0, 2, 0.51], [0, 3, 0.59], [0, 4, 0.07], [1, 2, 0.73], [1, 3, 0.3], [1, 4, 0.45], [2, 3, 0.98], [2, 4, 0.45], [3, 4, 0.61]], [[0, 1, 0.56], [0, 2, 0.75], [0, 3, 0.89], [0, 4, 0.91], [1, 2, 0.52], [1, 3, 0.44], [1, 4, 0.41], [2, 3, 0.91], [2, 4, 0.42], [3, 4, 0.61]], [[0, 1, 0.6], [0, 2, 0.68], [0, 3, 0.35], [0, 4, 0.57], [1, 2, 1.05], [1, 3, 0.74], [1, 4, 0.59], [2, 3, 0.34], [2, 4, 0.53], [3, 4, 0.37]], [[0, 1, 0.1], [0, 2, 1.2], [0, 3, 0.44], [0, 4, 0.45], [1, 2, 1.12], [1, 3, 0.38], [1, 4, 0.35], [2, 3, 0.77], [2, 4, 0.83], [3, 4, 0.29]], [[0, 1, 0.87], [0, 2, 0.32], [0, 3, 0.49], [0, 4, 0.73], [1, 2, 0.62], [1, 3, 0.64], [1, 4, 0.73], [2, 3, 0.53], [2, 4, 0.77], [3, 4, 0.25]], [[0, 1, 0.37], [0, 2, 0.37], [0, 3, 0.21], [0, 4, 0.61], [1, 2, 0.46], [1, 3, 0.58], [1, 4, 0.99], [2, 3, 0.53], [2, 4, 0.81], [3, 4, 0.42]], [[0, 1, 0.46], [0, 2, 0.98], [0, 3, 0.12], [0, 4, 0.14], [1, 2, 0.58], [1, 3, 0.38], [1, 4, 0.4], [2, 3, 0.94], [2, 4, 0.96], [3, 4, 0.04]], [[0, 1, 0.09], [0, 2, 0.16], [0, 3, 0.29], [0, 4, 0.73], [1, 2, 0.2], [1, 3, 0.33], [1, 4, 0.7], [2, 3, 0.43], [2, 4, 0.89], [3, 4, 0.55]], [[0, 1, 0.81], [0, 2, 0.54], [0, 3, 0.74], [0, 4, 0.89], [1, 2, 0.58], [1, 3, 0.23], [1, 4, 0.16], [2, 3, 0.37], [2, 4, 0.73], [3, 4, 0.39]], [[0, 1, 0.62], [0, 2, 0.67], [0, 3, 0.59], [0, 4, 0.99], [1, 2, 0.06], [1, 3, 0.5], [1, 4, 0.44], [2, 3, 0.5], [2, 4, 0.43], [3, 4, 0.93]], [[0, 1, 0.51], [0, 2, 0.82], [0, 3, 0.51], [0, 4, 0.44], [1, 2, 0.31], [1, 3, 0.7], [1, 4, 0.45], [2, 3, 0.91], [2, 4, 0.71], [3, 4, 0.9]], [[0, 1, 0.43], [0, 2, 0.65], [0, 3, 0.32], [0, 4, 0.31], [1, 2, 0.46], [1, 3, 0.63], [1, 4, 0.18], [2, 3, 0.62], [2, 4, 0.61], [3, 4, 0.58]], [[0, 1, 0.58], [0, 2, 0.58], [0, 3, 0.54], [0, 4, 0.58], [1, 2, 0.1], [1, 3, 1.11], [1, 4, 0.14], [2, 3, 1.11], [2, 4, 0.04], [3, 4, 1.1]], [[0, 1, 0.13], [0, 2, 0.65], [0, 3, 0.78], [0, 4, 0.74], [1, 2, 0.78], [1, 3, 0.8], [1, 4, 0.86], [2, 3, 0.76], [2, 4, 0.16], [3, 4, 0.68]], [[0, 1, 0.79], [0, 2, 0.3], [0, 3, 0.33], [0, 4, 0.59], [1, 2, 0.49], [1, 3, 0.88], [1, 4, 0.36], [2, 3, 0.5], [2, 4, 0.38], [3, 4, 0.57]], [[0, 1, 0.27], [0, 2, 0.29], [0, 3, 0.29], [0, 4, 0.47], [1, 2, 0.54], [1, 3, 0.38], [1, 4, 0.5], [2, 3, 0.51], [2, 4, 0.69], [3, 4, 0.18]], [[0, 1, 0.75], [0, 2, 0.45], [0, 3, 0.86], [0, 4, 0.65], [1, 2, 0.3], [1, 3, 0.17], [1, 4, 0.8], [2, 3, 0.42], [2, 4, 0.63], [3, 4, 0.78]], [[0, 1, 0.73], [0, 2, 0.13], [0, 3, 0.86], [0, 4, 0.7], [1, 2, 0.6], [1, 3, 0.16], [1, 4, 0.15], [2, 3, 0.73], [2, 4, 0.57], [3, 4, 0.3]], [[0, 1, 0.62], [0, 2, 0.24], [0, 3, 0.83], [0, 4, 0.44], [1, 2, 0.86], [1, 3, 0.89], [1, 4, 0.48], [2, 3, 0.92], [2, 4, 0.62], [3, 4, 0.46]], [[0, 1, 0.57], [0, 2, 0.45], [0, 3, 0.81], [0, 4, 0.37], [1, 2, 0.4], [1, 3, 0.66], [1, 4, 0.26], [2, 3, 0.98], [2, 4, 0.45], [3, 4, 0.53]], [[0, 1, 0.58], [0, 2, 0.54], [0, 3, 0.71], [0, 4, 0.57], [1, 2, 0.2], [1, 3, 0.3], [1, 4, 0.1], [2, 3, 0.5], [2, 4, 0.29], [3, 4, 0.22]], [[0, 1, 0.22], [0, 2, 0.61], [0, 3, 0.26], [0, 4, 0.21], [1, 2, 0.51], [1, 3, 0.21], [1, 4, 0.35], [2, 3, 0.73], [2, 4, 0.82], [3, 4, 0.24]], [[0, 1, 0.56], [0, 2, 0.77], [0, 3, 0.69], [0, 4, 0.46], [1, 2, 0.39], [1, 3, 0.64], [1, 4, 0.54], [2, 3, 0.41], [2, 4, 0.91], [3, 4, 1.02]], [[0, 1, 1.02], [0, 2, 0.32], [0, 3, 0.59], [0, 4, 0.7], [1, 2, 0.71], [1, 3, 0.58], [1, 4, 0.46], [2, 3, 0.42], [2, 4, 0.39], [3, 4, 0.59]], [[0, 1, 0.78], [0, 2, 0.78], [0, 3, 0.7], [0, 4, 0.61], [1, 2, 0.63], [1, 3, 0.63], [1, 4, 0.17], [2, 3, 0.09], [2, 4, 0.6], [3, 4, 0.58]], [[0, 1, 0.28], [0, 2, 0.28], [0, 3, 0.35], [0, 4, 0.52], [1, 2, 0.48], [1, 3, 0.59], [1, 4, 0.8], [2, 3, 0.11], [2, 4, 0.51], [3, 4, 0.44]], [[0, 1, 0.58], [0, 2, 0.1], [0, 3, 0.62], [0, 4, 0.39], [1, 2, 0.63], [1, 3, 0.73], [1, 4, 0.25], [2, 3, 0.55], [2, 4, 0.41], [3, 4, 0.51]], [[0, 1, 0.39], [0, 2, 0.4], [0, 3, 0.37], [0, 4, 0.24], [1, 2, 0.51], [1, 3, 0.59], [1, 4, 0.61], [2, 3, 0.13], [2, 4, 0.4], [3, 4, 0.31]], [[0, 1, 0.46], [0, 2, 0.61], [0, 3, 0.66], [0, 4, 0.49], [1, 2, 0.63], [1, 3, 0.34], [1, 4, 0.39], [2, 3, 0.97], [2, 4, 0.24], [3, 4, 0.73]], [[0, 1, 0.19], [0, 2, 0.61], [0, 3, 0.62], [0, 4, 0.71], [1, 2, 0.76], [1, 3, 0.8], [1, 4, 0.88], [2, 3, 0.2], [2, 4, 0.22], [3, 4, 0.08]], [[0, 1, 0.8], [0, 2, 0.74], [0, 3, 0.61], [0, 4, 0.59], [1, 2, 0.39], [1, 3, 0.36], [1, 4, 0.95], [2, 3, 0.13], [2, 4, 0.63], [3, 4, 0.59]], [[0, 1, 0.48], [0, 2, 1.16], [0, 3, 0.92], [0, 4, 0.86], [1, 2, 0.69], [1, 3, 0.46], [1, 4, 0.5], [2, 3, 0.28], [2, 4, 0.57], [3, 4, 0.31]], [[0, 1, 0.61], [0, 2, 0.52], [0, 3, 0.38], [0, 4, 0.82], [1, 2, 1.0], [1, 3, 0.31], [1, 4, 0.4], [2, 3, 0.87], [2, 4, 1.02], [3, 4, 0.67]], [[0, 1, 0.09], [0, 2, 0.49], [0, 3, 0.37], [0, 4, 0.77], [1, 2, 0.51], [1, 3, 0.44], [1, 4, 0.77], [2, 3, 0.3], [2, 4, 0.3], [3, 4, 0.59]], [[0, 1, 0.61], [0, 2, 0.72], [0, 3, 0.9], [0, 4, 0.56], [1, 2, 0.15], [1, 3, 0.55], [1, 4, 0.28], [2, 3, 0.65], [2, 4, 0.42], [3, 4, 0.37]], [[0, 1, 0.45], [0, 2, 0.64], [0, 3, 0.58], [0, 4, 0.47], [1, 2, 0.5], [1, 3, 0.39], [1, 4, 0.52], [2, 3, 0.11], [2, 4, 0.23], [3, 4, 0.25]], [[0, 1, 0.35], [0, 2, 0.62], [0, 3, 0.18], [0, 4, 0.32], [1, 2, 0.94], [1, 3, 0.51], [1, 4, 0.2], [2, 3, 0.55], [2, 4, 0.93], [3, 4, 0.42]], [[0, 1, 0.55], [0, 2, 0.7], [0, 3, 0.38], [0, 4, 0.64], [1, 2, 0.16], [1, 3, 0.75], [1, 4, 0.12], [2, 3, 0.91], [2, 4, 0.16], [3, 4, 0.79]], [[0, 1, 0.32], [0, 2, 0.31], [0, 3, 0.27], [0, 4, 0.39], [1, 2, 0.3], [1, 3, 0.38], [1, 4, 0.15], [2, 3, 0.54], [2, 4, 0.45], [3, 4, 0.35]], [[0, 1, 1.07], [0, 2, 0.79], [0, 3, 0.21], [0, 4, 0.91], [1, 2, 0.92], [1, 3, 0.92], [1, 4, 0.47], [2, 3, 0.85], [2, 4, 0.47], [3, 4, 0.85]], [[0, 1, 0.69], [0, 2, 0.04], [0, 3, 0.5], [0, 4, 0.69], [1, 2, 0.71], [1, 3, 1.1], [1, 4, 1.26], [2, 3, 0.52], [2, 4, 0.71], [3, 4, 0.19]], [[0, 1, 0.34], [0, 2, 0.58], [0, 3, 0.8], [0, 4, 0.41], [1, 2, 0.91], [1, 3, 1.03], [1, 4, 0.17], [2, 3, 0.43], [2, 4, 0.91], [3, 4, 0.96]], [[0, 1, 0.58], [0, 2, 0.71], [0, 3, 0.28], [0, 4, 0.39], [1, 2, 0.31], [1, 3, 0.71], [1, 4, 0.83], [2, 3, 0.73], [2, 4, 1.05], [3, 4, 0.62]], [[0, 1, 0.3], [0, 2, 0.57], [0, 3, 0.55], [0, 4, 0.8], [1, 2, 0.62], [1, 3, 0.65], [1, 4, 0.57], [2, 3, 0.1], [2, 4, 0.65], [3, 4, 0.74]], [[0, 1, 0.46], [0, 2, 0.7], [0, 3, 0.72], [0, 4, 0.33], [1, 2, 0.49], [1, 3, 0.43], [1, 4, 0.79], [2, 3, 0.11], [2, 4, 0.94], [3, 4, 0.98]], [[0, 1, 0.84], [0, 2, 0.08], [0, 3, 0.31], [0, 4, 0.5], [1, 2, 0.89], [1, 3, 0.75], [1, 4, 0.87], [2, 3, 0.28], [2, 4, 0.46], [3, 4, 0.22]], [[0, 1, 0.41], [0, 2, 0.18], [0, 3, 0.47], [0, 4, 0.38], [1, 2, 0.59], [1, 3, 0.87], [1, 4, 0.1], [2, 3, 0.3], [2, 4, 0.56], [3, 4, 0.85]], [[0, 1, 0.74], [0, 2, 0.7], [0, 3, 0.6], [0, 4, 0.76], [1, 2, 0.69], [1, 3, 0.21], [1, 4, 1.09], [2, 3, 0.76], [2, 4, 0.45], [3, 4, 1.1]], [[0, 1, 0.38], [0, 2, 0.39], [0, 3, 0.51], [0, 4, 0.67], [1, 2, 0.26], [1, 3, 0.14], [1, 4, 0.68], [2, 3, 0.28], [2, 4, 0.9], [3, 4, 0.77]]]

###########################################################################

###########################################################################
algorithm_config:                                       # config for QRL training
  reuse_actors:                   True
  num_gpus:                       0                     # number of gpus for each trial
  num_rollout_workers:            1                     # number of worker for each trial. By ray default, every worker needs one cpu
  num_envs_per_worker:            1                     # number of game enviroments for each worker
  num_gpus_per_worker:            0                     # number of gpus for each worker
  num_cpus_per_worker:            2                     # number of cpus for each worker
  framework:                      torch                 # ray framework [torch, tensorflow]
  ###########################################################################
  lr:                         0.001
  # weight_decay: 
  #   - grid_search
  #   - float
  #   - [0, 1.0e-4, 1.0e-5]
  lr_output_scaling:          0.1  
  num_layers:                   5            
    # - grid_search   
    # - int 
    # - [9, 12]
  
  target_network_update_freq: 5

  exploration_config:
    epsilon_timesteps: 5000
    final_epsilon: 0.01
    initial_epsilon: 1
    type: EpsilonGreedy

  replay_buffer_config:
    capacity: 10000
    replay_sequence_length: 1
    type: MultiAgentReplayBuffer
  
  num_steps_sampled_before_learning_starts: 100
  
  gamma: 0.99

  dueling: False

  double_q : False

  tau : 1

  td_error_loss_fn: mse

  grad_clip: None

  action_masking: True

  train_batch_size: 16
  # blocks: 
  #   - choice 
  #   - string
  #   - [[[[0, 1], [2, 3]], [[0, 1], [2], [3]]]]
  ###########################################################################
  mode:                           quantum               # select mode [classical, quantum, hybrid]
  interface:                      torch                 # select pennylane interface, default: torch
  diff_method:                    adjoint               # select pennylane diff_method [adjoing, backprop, ...] 
  backend_name:                   lightning.qubit       # select pennylane backend [lightning.qubit, default.qubit, ...]
  custom_optimizer:               Adam                  # select the classical optimizer [Adam, RMSprop, LBFGS, ...] 
  ###########################################################################
  vqc_type:                       [vqc_generator, 5]    # select vqc_generator or other circuit generator function + number of qubits
  use_hadamard:                   True                  # Create equal superposition in the beginning
  block_sequence:                 enc                   # select the block sequence, enc_var_ent == classical hwe ansatz, graph_encoding only needs enc
    # - grid_search
    # - string
    # - [enc, enc_var_ent]
  encoding_type:                  graph_encoding        # data encoding type [angular_classical (RY_RZ), layerwise_arctan_sigmoid, graph_encoding ... ]
  graph_encoding_type:            #s-ppgl          # if encoding_type=graph_encoding, than select [s-ppgl, m-ppgl, h-ppgl, hamiltonian-hwe, angular-hwe, angular, ...]
    - grid_search
    - string
    - [s-ppgl, m-ppgl, h-ppgl, s-ppgl-linear, m-ppgl-linear]
  use_input_scaling:              True                  # use input scaling [True, False] -> main parameters for graph encoding of eqc/neqc 
  init_input_scaling_actor:       [1.]                  # if list, then each gate gets one params, if single float, all have same param [[1.], 1., ...]
  num_scaling_params:             2                     # select the number of params, so e.g. 2 for angular_classical -> RY_RZ
  variational_type:               RZ_RY                 # select the gate sequence [RZ_RY, RY_RZ]
  num_variational_params:         2                     # select the number of params, so e.g. 2 for RZ_RY
  init_variational_params:        0.1                   # select initialization of the variational parameters
  init_variational_params_mode:   plus-zero-uniform     # plus-zero-uniform, plus-plus-normal, plus-zero-normal
  entangling_type:                chain                 # type of entanglement [chain, full, ...]
  entangling_gate:                CZ                    # type of entanglement gate [CNOT, CZ, CH, ...]
  measurement_type_actor:         exp                   # type of measurement (check the python files for examples) (exp for discrete) exp_@_exp+exp
  use_output_scaling_actor:       True                  # use output scaling [True, False]
  init_output_scaling_actor:      [1.]                  # if list, then each qubit gets one param, if single float, all have same param [[1.], 1., ...]
  postprocessing_actor:           1                     # select postprocessing (check the file postprocessing.py)
  ###########################################################################
  noise:                                                # use noise during training
    coherent:                     [False, 0.]           # bool + float for magnitude of used coherent noise
    depolarizing:                 [False, 0.001]        # bool + float for magnitude of used depolarizing noise
  layerwise_training:             False                 # layerwise training (DEPRECATED)
  gradient_clipping:              False                 # gradient clipping (DEPRECATED)
  use_classical_layer:            False                 # additional postprocessing (DEPRECATED)
  layer_size:                     [64, 64]              # classical NN, max 3 layers with in as number of neurons in the according layer
  activation_function:            relu                  # activation function of classical NN
  weight_logging_interval:        5000                  # weight logging + plotting interval (DEPRECATED)
  weight_plotting:                False                 # weight logging + plotting (DEPRECATED)
  ###########################################################################
  # More ray params
  explore:                        True
    # - grid_search
    # - string
    # - [False, True]
###########################################################################


evaluation:
  set_seed:                       True 
  seed:                           69
  ###########################################################################
  plotting:
    mode:                         custom
    path:                         logs/mvc_static/LinearRZ/4nodes_graph/2024-02-19--11-31-27_QRL_QPG
  ###########################################################################
  landscape_plotting:
    mode:                         qrl
    plot_name:                    qnn 4 qubits params=2 layer=1 
    plot_title:                   qaoa 4 qubits params=2 layer=1 
    qrl_config_path:              logs/qpg/uc/02_05/2024-02-05--12-06-29_QRL_QPG/
    plot_path:                    logs/qpg/uc/02_05/2024-02-05--12-06-29_QRL_QPG/ #logs/UC/dynamic/2024-02-07--14-27-34/
    center_params_path:           logs/qpg/uc/02_05/2024-02-05--12-06-29_QRL_QPG/QRL_PG_UC_demo_9cb93_00000_0_graph_encoding_type=eqc_2024-02-05_12-06-29/checkpoint_001000
                          # e.g. qrl: 'logs/qpg/uc/01_15/2024-01-17--11-39-09_QRL_QPG/QRL_PG_UC_demo_a552e_00000_0_graph_encoding_type=eqc,num_layers=3_2024-01-17_11-39-09/checkpoint_001000'
                          # e.g. qnn, qaoa, vqe: 'logs/comp/dynamic_5/2024-01-22--12-04-12/params_qnn_qubits_5_layer_5.csv'
    center_params:                trained_model         # can either be 0, trained_model
    evaluation_steps:             10                    # only required for qnn and qrl 
    vqc_type:                     qaoa                  # NOT FUNCTION YET! TODO: Implement Custom Circuit qaoa, vqe, qrl, custom_circuit
    num_layers:                   3                     # number of layers
    num_params:                   10                    # number of parameters per layers
    num_qubits:                   4                     # number of qubits
    num_div:                      10                    # Number of divisions to search (granularity of the grid / number of data points)
    scalor_1:                     1.0                   # set range for grid
    scalor_2:                     1.0                   # if grid is 2-dimensional, different scalors can be use e.g. for gamma and beta
    metric:                       accuracy              # z-axis for loss landscape for qrl [reward, accuracy], hard coded for qnn, vqe, qaoa
    mark_optimal:                 [True, 10]            # mark points which have same value as second list entry
    save_to_json:                 True                  # save losslandscape datapoints to json
  ###########################################################################
  algorithm_benchmarking:
    alg_to_train:                 []
    alg_to_evaluate:              [vqe, qaoa]
    qrl_path:                     logs/qpg/uc/02_09/2024-02-08--12-11-08_QRL_QPG/QRL_PG_UC_c280e_00000_0_graph_encoding_type=eqc,/checkpoint_001000
    qaoa_path:                    logs/UC/dynamic_10/2024-02-09--13-51-03/params_qaoa_qubits_10_layer_3_timestep_0.csv
    vqe_path:                     logs/UC/dynamic_10/2024-02-09--13-51-03/params_vqe_qubits_10_layer_3_timestep_0.csv
    optimizer_steps:              50
    evaluation_steps:             10
    num_qubits:                   10
    layer:
      layer_vqe:                  3
      layer_qaoa:                 3
      layer_qnn:                  3
    initialization:
      init_vqe:                   0.5
      init_qaoa:                  0.1
      init_qnn:                   0.1
    lr:
      lr_vqe:                     0.1
      lr_qaoa:                    0.01
      lr_qnn:                     0.1
    params_vqe:                   2
    params_qnn:                   10
    plotting:
      plot_vqe_probs:             True
      plot_qaoa_probs:            True
      plot_qnn_probs:             True                  # TODO
###########################################################################
  variance_calculation:   
    qubits:                       [2, 4, 6, 8, 10, 12, 14, 16]
    evaluations:                  100
    logging_path:                 logs/variance
    measurement_type_actor:       exp0
    calculate_vars:               False
    plot_vars:                    True
    save_to_json:                 True
    fig_name:                     variance calculation 
    path:                         /home/users/kruse/quantum-computing/QRL/logs/variance/2024-02-08--17-14-01_variances
    plot_keys:
      # - var_all_gradients
      - var_expecation
      # - var_weights_L
      # - var_input_scaling_L
 


  

  